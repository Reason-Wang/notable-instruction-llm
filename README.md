Collects model and data projects for instruction following large language models. These works may point out the future direction of natural language processing industry and research.

✕ means "not available"

**We welcome updates and suggestions**

| Project                                                      | model               | data                 | license    | note                                                         |
| ------------------------------------------------------------ | ------------------- | -------------------- | ---------- | :----------------------------------------------------------- |
| [stanford alpaca](https://github.com/tatsu-lab/stanford_alpaca) | llama               | alpaca               | apache 2.0 | Data is generated with `text-davinci-003`.                   |
| [AlpacaDataCleaned](https://github.com/gururise/AlpacaDataCleaned) | /                   | alpaca cleaned       | apache 2.0 | A cleaned and curated version of the Alpaca dataset.         |
| [ChatGLM](https://github.com/THUDM/ChatGLM-6B)               | GLM                 | ✕                    | apache 2.0 | A bilingual language model based on GLM framework.           |
| [flan-alpaca](https://github.com/declare-lab/flan-alpaca)    | Flan-T5             | alpaca               | apache 2.0 | Extend Standford Alpaca synthetic instruction tuning to Flan-T5. |
| [Dolly](https://github.com/databrickslabs/dolly)             | pythia              | databricks-dolly-15k | mit        | An instruction-following large language model trained on the Databricks machine learning platform that is licensed for commercial use. |
| [Vicuna](https://github.com/lm-sys/FastChat)                 | llama               | ShareGPT filtered, ✕ | apache 2.0 | User-shared conversations gathered from ShareGPT.com. Low-quality samples are filtered. |
| [ShareGPT unfiltered](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered) | /                   | ShareGPT unfiltered  | apache 2.0 | Conversations gathered from ShareGPT.com without filtering.  |
| [GPTeacher](https://github.com/teknium1/GPTeacher)           | /                   | GPTeacher            | mit        | A collection of modular datasets generated by GPT-4, General-Instruct - Roleplay-Instruct - Code-Instruct - and Toolformer. |
| [GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) | llama               | alpaca gpt4 data     | apache 2.0 | Instruction following data generated by GPT-4.               |
| [Luotuo](https://github.com/LC1332/Luotuo-Chinese-LLM)       | llama, chatglm, ... | unknown              | apache 2.0 | A set of Chinese instruction following large language models. |
| [Chinese-Vicuna](https://github.com/Facico/Chinese-Vicuna)   | llama               | BELLE                | apache 2.0 | Chinese instruction following large language model trained with lora. |
| [BELLE](https://github.com/LianjiaTech/BELLE)                | /                   | BELLE                | apache 2.0 | Chinese instruction data. Generated by ChatGPT using similar method as Alpaca. |

